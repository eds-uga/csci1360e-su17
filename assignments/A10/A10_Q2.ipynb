{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Q2\n",
    "\n",
    "In this question, we'll again look at classification, but with a more sophisticated algorithm. We'll also use the Iris dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part A\n",
    "\n",
    "In this question, you'll use a powerful classification technique known as *Support Vector Machines*, or SVMs.\n",
    "\n",
    "SVMs work by finding a line (or, in high dimensions, a hyperplane) that best separates data points that belong to different classes. SVMs are flexible enough to range from this fairly straightforward version, all the way to extremely complex incarnations that use nonlinear kernels to project the original data into extremely high-dimensional space, where (in theory) the data are easier to separate.\n",
    "\n",
    "SVMs can also enforce a penalty which \"allows\" for a certain amount of classification error, thereby making the decision boundary more fluid. This penalty term can be increased to make the decision boundary less permeable, or decreased to allow for more errors.\n",
    "\n",
    "In this part, you'll write code to train a linear SVM. In your code:\n",
    "\n",
    " - Define a function `train_svm()`.\n",
    " - `train_svm` should take 3 arguments: a data matrix `X`, a target array `y`, and a floating-point penalty strength term `C`.\n",
    " - It should return a trained SVM model.\n",
    "\n",
    "Your function should 1) create a linear SVM model, initialized with the correct penalty term, and 2) train (or *fit*) the model with a dataset and its labels. Look at the [scikit-learn documentation for Linear SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC).\n",
    "\n",
    "(The \"C\" in \"SVC\" means \"Support Vector **Classifier**, as scikit-learn also has SVM implementations that can be used for regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a_setup",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q2a_test1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13775)\n",
    "X = np.random.random((20, 2))\n",
    "y = np.random.randint(2, size = 20)\n",
    "\n",
    "m1 = train_svm(X, y, 100.0)\n",
    "assert m1.C == 100.0\n",
    "np.testing.assert_allclose(m1.coef_, np.array([[ 0.392707, -0.563687]]), rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q2a_test2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(598497)\n",
    "X = np.random.random((20, 2))\n",
    "y = np.random.randint(2, size = 20)\n",
    "\n",
    "m2 = train_svm(X, y, 10000.0)\n",
    "assert m2.C == 10000.0\n",
    "np.testing.assert_allclose(m2.coef_, np.array([[ -0.345056, -0.6118 ]]), rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part B\n",
    "\n",
    "In this part, you'll write an accompanying function to test the classification accuracy of your trained model.\n",
    "\n",
    "In your code:\n",
    "\n",
    " - Define a function `test_svm()`.\n",
    " - `test_svm` should take 3 arguments:  a data matrix `X`, a target array `y`, and a trained SVM model. It should return a prediction accuracy between 0 (completely incorrect) and 1 (completely correct).\n",
    " \n",
    "Your function can use the `score()` method available on the SVM model. Look at the [scikit-learn documentation for K-Nearest Neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q2b_test1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(58982)\n",
    "X = np.random.random((100, 2))\n",
    "y = np.random.randint(2, size = 100)\n",
    "\n",
    "m2 = train_svm(X[:75], y[:75], 100.0)\n",
    "acc2 = test_svm(X[75:], y[75:], m2)\n",
    "np.testing.assert_allclose(acc2, 0.36, rtol = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q2b_test2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(99766)\n",
    "X = np.random.random((20, 2))\n",
    "y = np.random.randint(2, size = 20)\n",
    "\n",
    "m2 = train_svm(X[:18], y[:18], 10.0)\n",
    "acc2 = test_svm(X[18:], y[18:], m2)\n",
    "np.testing.assert_allclose(acc2, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part C\n",
    "\n",
    "In this part, you'll test the functions you just wrote.\n",
    "\n",
    "The following code contains a *cross-validation loop*: it uses built-in scikit-learn tools to automate the task of implementing robust k-fold cross-validation. Incorporate your code into the core of the loop to extract sub-portions of the data matrix `X` and corresponding sub-portions of the target array `y` for training and testing.\n",
    "\n",
    "In the following code:\n",
    "\n",
    " - Implement training and testing of the SVM in each cross-validation loop. The point is that, in each loop, the training and testing sets are different than the previous loop.\n",
    " \n",
    " - Keep track of the average classification accuracy. Print it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q2c",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.cross_validation as cv\n",
    "\n",
    "# Set up the iris data.\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Some variables you're welcome to change, if you want.\n",
    "C = 1.0    # SVM penalty term\n",
    "folds = 5  # The \"k\" in \"k-fold cross-validation\"\n",
    "\n",
    "# Set up the cross-validation loop.\n",
    "kfold = cv.KFold(X.shape[0], n_folds = folds, shuffle = True, random_state = 10)\n",
    "for train, test in kfold:\n",
    "    # YOUR CODE HERE.\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part D\n",
    "\n",
    "How was your average classification accuracy in the previous question? How did that compare with the KNN accuracy from Q1? Does this difference or similiarity in accuracy make sense to you? Can you say anything about how the \"bias/variance tradeoff\" may or may not be at play here between KNN and SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2d",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
